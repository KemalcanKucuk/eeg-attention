{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import utils, selection, training, constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O - Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the feature dataset as a dataframe\n",
    "channel_names =  [\"Fp1\", \"Fp2\", \"F7\", \"F3\", \"Fz\", \"F4\", \"F8\", \"FT7\", \"FC3\", \"FCZ\", \"FC4\", \"FT8\", \"T3\", \"C3\", \"Cz\", \"C4\", \"T4\", \"TP7\", \"CP3\", \"CPz\", \"CP4\", \"TP8\", \"T5\", \"P3\", \"PZ\", \"P4\", \"T6\", \"O1\", \"Oz\" , \"O2\"]\n",
    "\n",
    "#channel subsets\n",
    "#test_channels_1 = [\"Cz\",\"CP3\",\"CPz\",\"P3\"]\n",
    "#test_channels_2 = [\"Fp1\", \"Fp2\", \"F7\", \"F3\", \"Fz\", \"F4\", \"F8\", \"FT7\", \"FC3\", \"FCZ\", \"FC4\", \"FT8\", \"T3\", \"C3\", \"Cz\", \"C4\", \"T4\", \"TP7\", \"CP3\", \"CPz\", \"CP4\", \"TP8\", \"T5\", \"P3\", \"PZ\", \"P4\", \"T6\", \"O1\", \"Oz\" , \"O2\"]\n",
    "#test_channels_3 = [\"Cz\",\"CP3\",\"CPz\",\"P3\",\"FT7\",\"FC3\",\"FCZ\",\"FC4\",\"C4\",\"CP4\",\"TP8\",\"T5\",\"PZ\",\"T6\",\"O1\",\"Oz\",\"O2\"]\n",
    "\n",
    "csv_file = 'eeg_features.csv'\n",
    "\n",
    "dataset = utils.data_loader(constants.MAIN_CSV_FILE)\n",
    "reduced_dataset_all = utils.channel_selection(dataset, channel_names)\n",
    "reduced_dataset_target = utils.channel_selection(dataset, constants.SELECTED_CHANNELS)\n",
    "#reduced_dataset_1 = utils.channel_selection(dataset, test_channels_1)\n",
    "#reduced_dataset_1 = utils.channel_selection(dataset, test_channels_1)\n",
    "#reduced_dataset_2 = utils.channel_selection(dataset, test_channels_2)\n",
    "#reduced_dataset_3 = utils.channel_selection(dataset, test_channels_3)\n",
    "\n",
    "#all_features = reduced_dataset_all.columns[:len(reduced_dataset_all.columns) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Channel Training+Incremental Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = incremental_training(dataset=dataset, channel_list=channel_list, feature_subset=all_features, models=['K-NN'], mode='feature', save=True)\n",
    "#calculate accuracy for each channel\n",
    "for channel in channel_names:\n",
    "    print(channel)\n",
    "    models = ['K-NN', 'GBC']\n",
    "    dataset = utils.data_loader(csv_file)\n",
    "    reduced_dataset = utils.channel_selection(dataset, [channel])\n",
    "    data = training.data_preparation(dataset=reduced_dataset, feature_subset=all_features)\n",
    "    for model in models:\n",
    "        model_training(data, model, stats=False, cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subsets = {}\n",
    "feature_subsets['bands'] = ['delta_power', 'theta_power',\n",
    "       'alpha_power', 'beta_power', 'gamma_power', 'gamma_beta', 'gamma_alpha',\n",
    "       'gamma_theta', 'gamma_delta', 'beta_alpha', 'beta_theta', 'beta_delta',\n",
    "       'alpha_theta', 'alpha_delta', 'theta_delta']\n",
    "\n",
    "feature_subsets['time_features'] = ['skew', 'kurtosis', 'rms', 'activity', 'mobility', 'complexity', 'dfa', 'mean_abs_sec_dif']\n",
    "\n",
    "feature_subsets['freq_features'] = ['spc_cnt', 'spc_roff', 'zc', 'slope']\n",
    "\n",
    "feature_subsets['mfcc_features'] = ['mfcc_0', 'mfcc_1', 'mfcc_2', 'mfcc_3',\n",
    "       'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9']\n",
    "\n",
    "feature_subsets['mel_features'] = ['mel_0', 'mel_1', 'mel_2',\n",
    "               'mel_3', 'mel_4', 'mel_5', 'mel_6', 'mel_7', 'mel_8', 'mel_9', 'mel_10',\n",
    "               'mel_11', 'mel_12', 'mel_13', 'mel_14']\n",
    "\n",
    "feature_subsets['chr_features'] = ['chr_0',\n",
    "                'chr_1', 'chr_2', 'chr_3', 'chr_4', 'chr_5', 'chr_6', 'chr_7', 'chr_8',\n",
    "                'chr_9', 'chr_10', 'chr_11', 'chr_12', 'chr_13', 'chr_14', 'chr_15',\n",
    "                'chr_16', 'chr_17', 'chr_18', 'chr_19']\n",
    "\n",
    "feature_subsets['ton_features'] = ['ton_0', 'ton_1', 'ton_2', 'ton_3', 'ton_4', 'ton_5']\n",
    "\n",
    "feature_subsets['spectral_features'] = feature_subsets['mfcc_features'] + feature_subsets['mel_features']\n",
    "\n",
    "feature_subsets['music'] = feature_subsets['chr_features'] + feature_subsets['ton_features']\n",
    "\n",
    "feature_subsets['coeffs'] = feature_subsets['spectral_features'] + feature_subsets['music']\n",
    "\n",
    "feature_subsets['comb_domain'] = feature_subsets['time_features'] + feature_subsets['freq_features'] + feature_subsets['bands']\n",
    "\n",
    "feature_subsets['no_music'] = feature_subsets['spectral_features'] + feature_subsets['comb_domain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = training.data_preparation(dataset=reduced_dataset_1, feature_subset=feature_subsets['no_music'])\n",
    "X = np.concatenate([X_train, X_test])\n",
    "X = scaler.fit_transform(X)\n",
    "y = np.concatenate([y_train, y_test]).flatten()\n",
    "\n",
    "chi2_stats, p_values = chi2(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_dict = {}\n",
    "for idx, feature in enumerate(feature_subsets['no_music']):\n",
    "    chi2_dict[feature] = chi2_stats[idx]\n",
    "chi_sorted = dict(sorted(chi2_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'complexity': 94.28211862279322,\n",
       " 'alpha_power': 61.61758765555503,\n",
       " 'spc_roff': 45.335745653110465,\n",
       " 'mel_10': 34.48564761491927,\n",
       " 'spc_cnt': 32.83364136602729,\n",
       " 'gamma_beta': 32.70304573869532,\n",
       " 'theta_power': 30.496223978846103,\n",
       " 'mel_7': 29.23799434939319,\n",
       " 'mel_8': 28.2197282588603,\n",
       " 'mel_11': 28.00341549280623,\n",
       " 'mel_14': 27.895614759311346,\n",
       " 'mel_6': 27.74221524291422,\n",
       " 'mel_12': 26.42683801615235,\n",
       " 'mel_9': 24.928265360565987,\n",
       " 'rms': 24.313244174897456,\n",
       " 'beta_alpha': 23.220564048041908,\n",
       " 'gamma_alpha': 22.58373946911418,\n",
       " 'mel_13': 21.773202922383213,\n",
       " 'alpha_delta': 20.175210830953596,\n",
       " 'mfcc_8': 19.043161553563458,\n",
       " 'mobility': 17.02425805895643,\n",
       " 'mel_5': 16.826754580813095,\n",
       " 'mfcc_2': 16.639397142777653,\n",
       " 'mfcc_3': 14.875414600070165,\n",
       " 'mel_4': 12.497447896982823,\n",
       " 'mfcc_6': 11.937535938453392,\n",
       " 'alpha_theta': 11.095610187638918,\n",
       " 'mfcc_0': 10.90100267891954,\n",
       " 'mel_2': 9.799091830858487,\n",
       " 'mel_3': 9.642782891854505,\n",
       " 'mel_1': 9.174846537131351,\n",
       " 'gamma_theta': 9.112422068721376,\n",
       " 'delta_power': 8.70172613116474,\n",
       " 'mean_abs_sec_dif': 8.026987284463438,\n",
       " 'zc': 7.055753967309027,\n",
       " 'mfcc_1': 6.131827007644796,\n",
       " 'gamma_delta': 5.964851215484057,\n",
       " 'mel_0': 5.759426733627833,\n",
       " 'activity': 5.6993155650467955,\n",
       " 'dfa': 5.5830366851863005,\n",
       " 'mfcc_5': 4.48748047893481,\n",
       " 'beta_theta': 4.3722495094339475,\n",
       " 'theta_delta': 2.9816120211729182,\n",
       " 'mfcc_4': 2.106270968103871,\n",
       " 'beta_power': 2.089149259415795,\n",
       " 'mfcc_7': 1.8565982059136972,\n",
       " 'gamma_power': 0.8955821607570245,\n",
       " 'mfcc_9': 0.28175269168179806,\n",
       " 'skew': 0.2535951476978645,\n",
       " 'slope': 0.050539562262146856,\n",
       " 'beta_delta': 0.005567609826478444,\n",
       " 'kurtosis': 0.00011142454858877019}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_df, variance_dict = selection.variance_thresholding(reduced_dataset_1, threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = training.data_preparation(dataset=reduced_dataset_1, feature_subset=feature_subsets['no_music'])\n",
    "X = np.concatenate([X_train, X_test])\n",
    "X = scaler.fit_transform(X)\n",
    "y = np.concatenate([y_train, y_test]).flatten()\n",
    "\n",
    "f_statistic, p_values = f_classif(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dict = {}\n",
    "for idx, feature in enumerate(feature_subsets['no_music']):\n",
    "    f_dict[feature] = f_statistic[idx]\n",
    "f_sorted = dict(sorted(f_dict.items(), key=lambda item: item[1], reverse=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spc_roff': 1179.7978846086457,\n",
       " 'complexity': 1171.7471438323246,\n",
       " 'mel_11': 1160.4100113966963,\n",
       " 'mel_10': 1144.9544212153526,\n",
       " 'mel_6': 1075.6277307751693,\n",
       " 'mel_7': 952.5670299279793,\n",
       " 'mel_12': 942.540337366972,\n",
       " 'mel_9': 936.5967676403434,\n",
       " 'mel_8': 891.0888782245993,\n",
       " 'mel_14': 820.7423233308908,\n",
       " 'mel_13': 764.8618977509211,\n",
       " 'rms': 724.8313528466728,\n",
       " 'spc_cnt': 717.5656346396459,\n",
       " 'mel_5': 707.6203337209621,\n",
       " 'alpha_power': 674.0351116859397,\n",
       " 'theta_power': 659.5643477476898,\n",
       " 'mfcc_8': 541.380384782955,\n",
       " 'mfcc_2': 457.90180619717114,\n",
       " 'gamma_beta': 427.4471517208908,\n",
       " 'mel_4': 384.0438940190917,\n",
       " 'mfcc_3': 360.71091534674434,\n",
       " 'mel_3': 349.0845613165568,\n",
       " 'mobility': 347.5485006669642,\n",
       " 'mfcc_0': 332.5889617865878,\n",
       " 'mfcc_6': 326.9010075256861,\n",
       " 'mel_2': 303.7056483738949,\n",
       " 'mel_1': 281.4245060104526,\n",
       " 'zc': 265.22149867835975,\n",
       " 'mean_abs_sec_dif': 256.20641049970425,\n",
       " 'dfa': 254.07184268421352,\n",
       " 'beta_alpha': 249.93641970469574,\n",
       " 'alpha_delta': 223.3189549940299,\n",
       " 'mfcc_1': 222.73801619514452,\n",
       " 'gamma_alpha': 221.68789062284478,\n",
       " 'mel_0': 206.97428757132352,\n",
       " 'delta_power': 189.24747385962695,\n",
       " 'alpha_theta': 140.99860515145866,\n",
       " 'gamma_theta': 139.227799134458,\n",
       " 'activity': 124.01221602503415,\n",
       " 'mfcc_5': 114.55056121277767,\n",
       " 'gamma_delta': 87.2479717194567,\n",
       " 'mfcc_4': 85.58181451221179,\n",
       " 'beta_theta': 76.57521864315568,\n",
       " 'skew': 61.96560616320828,\n",
       " 'beta_power': 58.20123475753122,\n",
       " 'theta_delta': 56.425180697711724,\n",
       " 'mfcc_7': 51.31978952080081,\n",
       " 'slope': 32.351955830796285,\n",
       " 'mfcc_9': 13.583291383945342,\n",
       " 'gamma_power': 9.381093678486982,\n",
       " 'beta_delta': 0.06262726708045808,\n",
       " 'kurtosis': 0.003441500200284913}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Stats_dict for the K-NN model ====\n",
      "Training Accuracy:  0.7527047913446677\n",
      "Test Accuracy:  0.7435105067985167\n",
      "Sensitivity (Recall): 0.7391304347826086\n",
      "Precision: 0.74375\n",
      "F1_score: 0.7414330218068534\n",
      "AUC: 0.7434889566286967\n",
      "Logloss: 9.244818390904586\n",
      "\n",
      "\n",
      "==== Stats_dict for the SVM model ====\n",
      "Training Accuracy:  1.0\n",
      "Test Accuracy:  0.927070457354759\n",
      "Sensitivity (Recall): 0.9465838509316771\n",
      "Precision: 0.910394265232975\n",
      "F1_score: 0.9281364190012181\n",
      "AUC: 0.9271664642112261\n",
      "Logloss: 2.6286471569319065\n",
      "\n",
      "\n",
      "==== Stats_dict for the DTC model ====\n",
      "Training Accuracy:  0.8360123647604327\n",
      "Test Accuracy:  0.7571075401730532\n",
      "Sensitivity (Recall): 0.7751552795031056\n",
      "Precision: 0.7464114832535885\n",
      "F1_score: 0.7605118829981717\n",
      "AUC: 0.7571963359385147\n",
      "Logloss: 8.754731632832534\n",
      "\n",
      "\n",
      "==== Stats_dict for the RFC model ====\n",
      "Training Accuracy:  1.0\n",
      "Test Accuracy:  0.8331273176761433\n",
      "Sensitivity (Recall): 0.8596273291925466\n",
      "Precision: 0.8150765606595995\n",
      "F1_score: 0.8367593712212817\n",
      "AUC: 0.8332576990366177\n",
      "Logloss: 6.014701121793346\n",
      "\n",
      "\n",
      "==== Stats_dict for the LR model ====\n",
      "Training Accuracy:  0.7678516228748068\n",
      "Test Accuracy:  0.7472187886279357\n",
      "Sensitivity (Recall): 0.7639751552795031\n",
      "Precision: 0.737410071942446\n",
      "F1_score: 0.7504575960951799\n",
      "AUC: 0.7473012307762829\n",
      "Logloss: 9.111158365975843\n",
      "\n",
      "\n",
      "==== Stats_dict for the NN model ====\n",
      "Training Accuracy:  0.9901081916537867\n",
      "Test Accuracy:  0.8257107540173053\n",
      "Sensitivity (Recall): 0.8260869565217391\n",
      "Precision: 0.8240396530359355\n",
      "F1_score: 0.825062034739454\n",
      "AUC: 0.8257126049521366\n",
      "Logloss: 6.282021171650826\n",
      "\n",
      "\n",
      "==== Stats_dict for the GBC model ====\n",
      "Training Accuracy:  1.0\n",
      "Test Accuracy:  0.8473423980222496\n",
      "Sensitivity (Recall): 0.8645962732919255\n",
      "Precision: 0.8345323741007195\n",
      "F1_score: 0.8492983526540574\n",
      "AUC: 0.8474272879374757\n",
      "Logloss: 5.502337692899837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anova_selected = ['spc_roff', 'complexity', 'mel_11', 'mel_10', 'mel_6', 'mel_7', 'mel_12', 'mel_9', 'mel_8', 'mel_14', 'mel_13', 'rms', 'spc_cnt', 'mel_5', 'alpha_power', 'theta_power', 'mfcc_8', 'mfcc_2', 'gamma_beta', 'mel_4', 'mfcc_3', 'mel_3', 'mobility', 'mfcc_0', 'mfcc_6', 'mel_2', 'mel_1', 'zc', 'mean_abs_sec_dif', 'dfa', 'beta_alpha', 'alpha_delta', 'mfcc_1', 'gamma_alpha', 'mel_0', 'delta_power', 'alpha_theta', 'gamma_theta', 'activity', 'mfcc_5', 'gamma_delta', 'mfcc_4', 'beta_theta', 'skew', 'beta_power', 'theta_delta', 'mfcc_7', 'slope']\n",
    "data = training.data_preparation(dataset=reduced_dataset_target, feature_subset=anova_selected)\n",
    "for model in constants.ALL_MODELS:\n",
    "    training.model_training(data, model, stats=True, cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Value Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kemalcankucuk/Documents/PAWS Work/eeg-attention/selection.py:77: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p_value = stats.ttest_ind(\n",
      "/Users/kemalcankucuk/Documents/PAWS Work/eeg-attention/selection.py:77: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p_value = stats.ttest_ind(\n",
      "/Users/kemalcankucuk/Documents/PAWS Work/eeg-attention/selection.py:77: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p_value = stats.ttest_ind(\n",
      "/Users/kemalcankucuk/Documents/PAWS Work/eeg-attention/selection.py:77: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p_value = stats.ttest_ind(\n"
     ]
    }
   ],
   "source": [
    "channel_subsets = [channel_names, test_channels_1, test_channels_2, test_channels_3]\n",
    "subset_names = ['chn_all', 'chn_1', 'chn_2', 'chn_3']\n",
    "p_val_df = pd.DataFrame(columns=subset_names)\n",
    "\n",
    "for i in range(len(channel_subsets)):\n",
    "    dataset = data_loader(csv_file)\n",
    "    reduced_dataset_i = utils.channel_selection(dataset, channel_subsets[i])\n",
    "    p_i, p_i_val = selcetion.p_value_thresholding(reduced_dataset_i, feature_subset=all_features)\n",
    "    p_val_df[subset_names[i]] = p_i_val    \n",
    "\n",
    "p_val_df.to_csv('outs/p_values_by_channels.csv')\n",
    "\n",
    "a='''\n",
    "p_all, p_all_val = p_value_thresholding(reduced_dataset_all, feature_subset=all_features)\n",
    "    p_1, p_1_val = p_value_thresholding(reduced_dataset_1, feature_subset=all_features)\n",
    "    p_2, p_2_val = p_value_thresholding(reduced_dataset_2, feature_subset=all_features)\n",
    "    p_3, p_3_val = p_value_thresholding(reduced_dataset_3, feature_subset=all_features)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yurteri's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_better = []\n",
    "for i in range(len(accuracies) - 1):\n",
    "    delta = accuracies[i+1] - accuracies[i]\n",
    "    if delta <= 0:\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        p_better.append(p_all[i])\n",
    "\n",
    "data = data_preparation(selected_channels=selected_channels, selected_labels=selected_labels, feature_subset=p_better)\n",
    "for model in models:\n",
    "    training, test = model_training(data, model, stats=False, cm=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.linspace(1,len(accuracies),len(accuracies)), accuracies)\n",
    "plt.legend()\n",
    "plt.savefig('foo.png', bbox_inches='tight')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.legend(['GBC', 'K-NN', 'SVM', 'DTC', 'NN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReliefF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReliefF import ReliefF\n",
    "dataset = utils.data_loader(csv_file)\n",
    "best_channel_list = ['CP3', 'Cz', 'CPz', 'P3']\n",
    "reduced_dataset = utils.channel_selection(dataset, best_channel_list)\n",
    "X_train, X_test, y_train, y_test = training.data_preparation(dataset=reduced_dataset, feature_subset=all_features)\n",
    "X = np.concatenate([X_train, X_test])\n",
    "y = np.concatenate([y_train, y_test]).flatten()\n",
    "fs = ReliefF(n_neighbors=1, n_features_to_keep=79)\n",
    "rf = fs.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_scores = pd.DataFrame(columns=dataset.columns[1:80])\n",
    "for idx, col in enumerate(rf_scores.columns):\n",
    "    rf_scores[col] = rf[:][idx]\n",
    "rf_scores.to_csv('rf scores.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
