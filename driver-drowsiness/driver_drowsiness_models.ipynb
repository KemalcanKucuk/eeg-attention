{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from ipynb.fs.full.driver_drowsiness_extraction import select_channel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_selection(features, labels, channel_list):\n",
    "    '''\n",
    "    Select the desired channels from the total feature dataset\n",
    "    '''\n",
    "    selected_channels = []\n",
    "    for channel in channel_list:\n",
    "        selected_channels.append(features.loc[features['channels'] == channel])\n",
    "    # return the corresponding labels for the selected channels\n",
    "    selected_labels = labels[0:2022*len(channel_list)].to_numpy()\n",
    "    return ((pd.concat(selected_channels).drop('channels', axis=1)), selected_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(selected_channels, feature_subset):\n",
    "    ''' \n",
    "    Select the desired subset of features to prepare training data on.\n",
    "    '''\n",
    "    selected_features = pd.DataFrame()\n",
    "    for feature in feature_subset:\n",
    "        selected_features[feature] = selected_channels[feature]\n",
    "    # temporary sanity check, will delete\n",
    "    # print(selected_features.head())\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(selected_channels, selected_labels, feature_subset, split_size = 0.2, seed = 1):\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    X = feature_selection(selected_channels=selected_channels, feature_subset = feature_subset) # select every feature\n",
    "    y = selected_labels\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split_size, random_state = seed)\n",
    "\n",
    "    # apply normalization after splitting to avoid leakage\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "def model_training(data, model_family, display_labels, verbose = True, stats=False, cm=False):\n",
    "\n",
    "  X_train, X_test, y_train, y_test = data\n",
    "  if model_family == 'K-NN':\n",
    "    model = KNeighborsClassifier()\n",
    "  elif model_family == 'DTC':\n",
    "    model = DecisionTreeClassifier()\n",
    "  elif model_family == 'RFC':\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "  elif model_family == 'Logistic Regression':\n",
    "    model = LogisticRegression(max_iter=5000)\n",
    "  elif model_family == 'SVM':\n",
    "    model = SVC(C=1.0, kernel='rbf', degree=10, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=1)\n",
    "  elif model_family == 'NN':\n",
    "    model = MLPClassifier(activation='relu',solver='adam', alpha=1e-2, learning_rate='adaptive', max_iter=1000000, hidden_layer_sizes=(60,2), random_state=1)\n",
    "  elif model_family == 'GBC':\n",
    "    model = GradientBoostingClassifier(loss='log_loss',n_estimators=300, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "\n",
    "  model.fit(X_train, y_train)\n",
    "  training_acc = model.score(X_train, y_train)\n",
    "  test_acc = model.score(X_test, y_test)\n",
    "  if verbose:\n",
    "    print('Accuracy of {} classifier on training set: {:.8f}'\n",
    "      .format(model_family, training_acc))\n",
    "    print('Accuracy of {} classifier on test set: {:.8f}'\n",
    "      .format(model_family, test_acc))\n",
    "\n",
    "  if stats:\n",
    "    print()\n",
    "    print(\"==== Stats for the {} model ====\".format(model_family))\n",
    "    sensitivity = recall_score(y_test, model.predict(X_test))\n",
    "    print(\"Sensitivity (Recall):\", sensitivity)\n",
    "\n",
    "    precision = precision_score(y_test, model.predict(X_test))\n",
    "    print(\"Precision:\", precision)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(\"Accuracy (Recall):\", accuracy)\n",
    "        \n",
    "    f1 = f1_score(y_test, model.predict(X_test))\n",
    "    print(\"F1_score:\", f1)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, model.predict(X_test))\n",
    "    auc = roc_auc_score(y_test, model.predict(X_test))\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    logloss = log_loss(y_test, model.predict(X_test))\n",
    "    print(\"Logloss:\", logloss)\n",
    "    print()\n",
    "\n",
    "  if cm:\n",
    "    model_cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    model_disp = ConfusionMatrixDisplay(confusion_matrix=model_cm,display_labels=display_labels)\n",
    "    model_disp.plot()\n",
    "  \n",
    "  return [training_acc, test_acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the feature dataset as a dataframe\n",
    "csv_file = 'eeg_features.csv'\n",
    "df = pd.read_csv(csv_file,float_precision='round_trip')\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset to features and labels\n",
    "features = df.drop('label', axis=1)\n",
    "labels = df.iloc[:,-1:]\n",
    "display_labels = ['drowsy' if label == 1 else 'alert' for label in labels['label'].unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def feature_combination(feature_subset, min_n = 1, max_n = 5, training=False, pvalue=False):\n",
    "    '''\n",
    "    Go through a feature subset and calculate the combinations of different features on the subsets.\n",
    "    '''\n",
    "    filename = str(feature_subset) + '_' + str(min_n) + 'to' + str(max_n) + '.txt'\n",
    "    file = open(filename, 'w')\n",
    "    # selected channels and labels are global now, might fix it if we decide settling on this method.\n",
    "    for i in range(min_n, max_n):\n",
    "        for comb in list(itertools.combinations(feature_subset, i)):\n",
    "            if training:\n",
    "                data = data_preparation(selected_channels=selected_channels, selected_labels=selected_labels, feature_subset=comb)\n",
    "                # parametrize the models\n",
    "                for model in ['K-NN', 'SVM']:\n",
    "                    train_acc, test_acc = model_training(data, model, display_labels, verbose=False, stats=False, cm=False)\n",
    "                    # TODO: improve the readability of the file by either modifying the string or changing the filetype entirely\n",
    "                    file.writelines(f\"{comb}: {model} train acc: {train_acc:.2f} test acc: {test_acc:.2f}\\n\")\n",
    "            # we can also add a thresholding section to see if the pvalue or variance value changes\n",
    "            if pvalue:\n",
    "                pass\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with spectral features and their combinations\n",
    "feature_combination(['spc_cnt', 'spc_roff','slope', 'mfcc_0', 'mfcc_1', 'mfcc_2', 'mfcc_3'], min_n = 2, max_n = 5, training=True, pvalue=False)\n",
    "\n",
    "# if you wanna test your limits\n",
    "'''\n",
    "min_n = 2, max_n = 50 # did i ever tell you what the definition of insanity is?\n",
    "feature_combination(selected_channels.columns, min_n = min_n, max_n = max_n, training=True, pvalue=False)\n",
    "'''\n",
    "\n",
    "# if you feel lucky\n",
    "'''\n",
    "import random\n",
    "subset_size = 10 # go wild\n",
    "random_subset = random.sample(selected_channels.columns, subset_size)\n",
    "feature_combination(random_subset, min_n = 2, max_n = 5, training=True, pvalue=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the channels to be processed\n",
    "channel_list = ['F3', 'F4','C3','Cz','Oz']\n",
    "\n",
    "# select the models to be trained\n",
    "models = ['GBC', 'K-NN', 'SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GBC classifier on training set: 0.99950544\n",
      "Accuracy of GBC classifier on test set: 0.64886251\n",
      "\n",
      "==== Stats for the GBC model ====\n",
      "Sensitivity (Recall): 0.6449704142011834\n",
      "Precision: 0.651394422310757\n",
      "Accuracy (Recall): 0.6488625123639961\n",
      "F1_score: 0.6481665014866204\n",
      "AUC: 0.6488740959894806\n",
      "Logloss: 12.656277896277537\n",
      "\n",
      "Accuracy of K-NN classifier on training set: 0.72143917\n",
      "Accuracy of K-NN classifier on test set: 0.55736894\n",
      "\n",
      "==== Stats for the K-NN model ====\n",
      "Sensitivity (Recall): 0.5512820512820513\n",
      "Precision: 0.5595595595595596\n",
      "Accuracy (Recall): 0.5573689416419386\n",
      "F1_score: 0.5553899652260309\n",
      "AUC: 0.5573870573870574\n",
      "Logloss: 15.954040446716048\n",
      "\n",
      "Accuracy of SVM classifier on training set: 0.62685460\n",
      "Accuracy of SVM classifier on test set: 0.63353116\n",
      "\n",
      "==== Stats for the SVM model ====\n",
      "Sensitivity (Recall): 0.6045364891518737\n",
      "Precision: 0.6432318992654774\n",
      "Accuracy (Recall): 0.6335311572700296\n",
      "F1_score: 0.623284189120488\n",
      "AUC: 0.6336174509251431\n",
      "Logloss: 13.208875945269934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_channels, selected_labels = channel_selection(features=features, labels=labels, channel_list=channel_list)\n",
    "data = data_preparation(selected_channels=selected_channels, selected_labels=selected_labels, feature_subset=['chr_11', 'chr_14', 'chr_12', 'mean_abs_sec_dif', 'delta_power'])\n",
    "\n",
    "for model in models:\n",
    "    model_training(data, model, display_labels, stats=True, cm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-Value Thresholding for Feature Selection\n",
    "def p_value_thresholding(selected_features, selected_labels):\n",
    "\n",
    "    p_values = []\n",
    "\n",
    "    X_p = selected_features\n",
    "    \n",
    "    y_p = selected_labels.flatten()\n",
    "    y_p = pd.Series(y_p)\n",
    "\n",
    "\n",
    "    #y_p = pd.Series(y['0'])\n",
    "    sorted_dict = {}\n",
    "    for feature in X_p.columns:\n",
    "        t_stat, p_value = stats.ttest_ind(X_p[feature][y_p == 0], X_p[feature][y_p == 1])\n",
    "        p_values.append(p_value)\n",
    "        sorted_dict[feature] = p_value\n",
    "\n",
    "\n",
    "    alpha = 0.05\n",
    "\n",
    "    # Select features with p-values below the significance level\n",
    "    selected_features = [X_p.columns[i] for i, p in enumerate(p_values) if p < alpha]\n",
    "    # Alternatively, you can rank features by p-value\n",
    "    sorted_features = [x for _, x in sorted(zip(p_values, X_p.columns))]\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    ordered = OrderedDict(sorted(sorted_dict.items(), key=lambda item:np.max(item[1])))\n",
    "    for key, value in ordered.items():\n",
    "        print(key, value)\n",
    "        \n",
    "    return sorted_features, sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spc_roff 1.6147525601572645e-241\n",
      "slope 2.1961591318448354e-218\n",
      "mel_9 3.780620351407701e-197\n",
      "mel_6 1.1018938972585085e-190\n",
      "mel_8 7.18754692852844e-179\n",
      "mel_7 4.889762489252905e-178\n",
      "spc_cnt 2.887779599226387e-170\n",
      "mel_5 5.517438027558447e-134\n",
      "mfcc_2 3.901885014029416e-112\n",
      "zc 8.621303483438252e-98\n",
      "mel_1 7.514574188776398e-96\n",
      "mel_2 4.0251470438104993e-85\n",
      "mel_0 7.866398310131055e-85\n",
      "mel_3 7.917307252001706e-84\n",
      "dfa 5.4570769344308075e-80\n",
      "mel_4 4.070881376943928e-78\n",
      "gamma_beta 1.9635186392167987e-75\n",
      "chr_9 1.0813929454262362e-62\n",
      "chr_8 7.157621087120184e-59\n",
      "gamma_alpha 1.8360025005449308e-58\n",
      "mfcc_1 5.293632675036034e-58\n",
      "mfcc_0 8.189398043909328e-58\n",
      "mfcc_3 9.75984225924415e-57\n",
      "alpha_delta 2.827046503995087e-54\n",
      "alpha_theta 7.560080511524009e-42\n",
      "chr_10 5.609345536289729e-37\n",
      "peak_freq 1.2178319684208423e-34\n",
      "beta_alpha 1.825251593945194e-32\n",
      "mfcc_4 5.3463501550803495e-24\n",
      "chr_7 3.0341498587256606e-21\n",
      "gamma_delta 6.846410819985025e-20\n",
      "gamma_theta 1.0310558268088689e-17\n",
      "beta_theta 2.5202117308661734e-14\n",
      "chr_1 2.104710238944142e-12\n",
      "chr_2 4.154489088864628e-12\n",
      "chr_0 3.5105356269465397e-11\n",
      "theta_delta 3.9243176168412524e-10\n",
      "chr_3 2.267357861996565e-06\n",
      "beta_delta 5.681383911087017e-06\n",
      "chr_4 0.012346537048946126\n",
      "alpha_power 0.014917544367897531\n",
      "max_freq 0.03018912494211042\n",
      "gamma_power 0.07414771996759419\n",
      "beta_power 0.14431535081612742\n",
      "chr_5 0.15610899401813372\n",
      "chr_6 0.24871448828958442\n",
      "theta_power 0.25723500151888484\n",
      "chr_13 0.31891612858337426\n",
      "chr_12 0.32485418779189656\n",
      "delta_power 0.3286381004740232\n",
      "chr_14 0.35313785017749466\n",
      "chr_11 0.7513748226692727\n",
      "mean_abs_sec_dif 0.7896583330703082\n"
     ]
    }
   ],
   "source": [
    "all = feature_selection(selected_channels=selected_channels, feature_subset=selected_channels.columns) # select every feature\n",
    "p_all, p_dict = p_value_thresholding(selected_features=all, selected_labels=y)\n",
    "\n",
    "#print(p_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spc_roff',\n",
       " 'slope',\n",
       " 'mel_9',\n",
       " 'mel_6',\n",
       " 'mel_8',\n",
       " 'mel_7',\n",
       " 'spc_cnt',\n",
       " 'mel_5',\n",
       " 'mfcc_2',\n",
       " 'zc',\n",
       " 'mel_1',\n",
       " 'mel_2',\n",
       " 'mel_0',\n",
       " 'mel_3',\n",
       " 'dfa',\n",
       " 'mel_4',\n",
       " 'gamma_beta',\n",
       " 'chr_9',\n",
       " 'chr_8',\n",
       " 'gamma_alpha',\n",
       " 'mfcc_1',\n",
       " 'mfcc_0',\n",
       " 'mfcc_3',\n",
       " 'alpha_delta',\n",
       " 'alpha_theta',\n",
       " 'chr_10',\n",
       " 'peak_freq',\n",
       " 'beta_alpha',\n",
       " 'mfcc_4',\n",
       " 'chr_7',\n",
       " 'gamma_delta',\n",
       " 'gamma_theta',\n",
       " 'beta_theta',\n",
       " 'chr_1',\n",
       " 'chr_2',\n",
       " 'chr_0',\n",
       " 'theta_delta',\n",
       " 'chr_3',\n",
       " 'beta_delta',\n",
       " 'chr_4',\n",
       " 'alpha_power',\n",
       " 'max_freq',\n",
       " 'gamma_power',\n",
       " 'beta_power',\n",
       " 'chr_5',\n",
       " 'chr_6',\n",
       " 'theta_power',\n",
       " 'chr_13',\n",
       " 'chr_12',\n",
       " 'delta_power',\n",
       " 'chr_14',\n",
       " 'chr_11',\n",
       " 'mean_abs_sec_dif']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['spc_roff',\n",
    " 'mfcc_0',\n",
    " 'spc_cnt',\n",
    " 'mfcc_1',\n",
    " 'mfcc_2',\n",
    " 'zc',\n",
    " 'dfa',\n",
    " 'gamma_beta',\n",
    " 'mfcc_3',\n",
    " 'mel_4',\n",
    " 'mel_7',\n",
    " 'chr_3',\n",
    " 'chr_2',\n",
    " 'chr_7',\n",
    " 'mel_5',\n",
    " 'chr_4',\n",
    " 'chr_5',\n",
    " 'mel_6',\n",
    " 'chr_1',\n",
    " 'chr_6',\n",
    " 'mel_3',\n",
    " 'mel_0',\n",
    " 'mel_1',\n",
    " 'mel_2',\n",
    " 'gamma_alpha',\n",
    " 'chr_0',\n",
    " 'chr_8',\n",
    " 'mel_8',\n",
    " 'alpha_delta',\n",
    " 'mel_9',\n",
    " 'alpha_theta',\n",
    " 'chr_14',\n",
    " 'beta_alpha',\n",
    " 'chr_9',\n",
    " 'mfcc_4',\n",
    " 'gamma_delta',\n",
    " 'gamma_theta',\n",
    " 'chr_10',\n",
    " 'beta_theta',\n",
    " 'chr_11',\n",
    " 'theta_delta',\n",
    " 'beta_delta',\n",
    " 'chr_13',\n",
    " 'chr_12',\n",
    " 'alpha_power',\n",
    " 'gamma_power',\n",
    " 'beta_power',\n",
    " 'theta_power',\n",
    " 'delta_power',\n",
    " 'mean_abs_sec_dif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "#y = y.reset_index(drop=True)\n",
    "\n",
    "pca = PCA(n_components = 0.999)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "#X = dataPCA\n",
    "variance = pd.DataFrame(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
