{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ee8b672",
   "metadata": {
    "papermill": {
     "duration": 0.023843,
     "end_time": "2022-09-22T07:22:46.443323",
     "exception": false,
     "start_time": "2022-09-22T07:22:46.419480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EEG Feature Extraction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7131706d",
   "metadata": {
    "papermill": {
     "duration": 0.018896,
     "end_time": "2022-09-22T07:22:46.482468",
     "exception": false,
     "start_time": "2022-09-22T07:22:46.463572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing all required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40b87dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T07:22:46.540103Z",
     "iopub.status.busy": "2022-09-22T07:22:46.539024Z",
     "iopub.status.idle": "2022-09-22T07:22:47.628484Z",
     "shell.execute_reply": "2022-09-22T07:22:47.627610Z",
     "shell.execute_reply.started": "2022-09-22T07:12:02.806360Z"
    },
    "papermill": {
     "duration": 1.125879,
     "end_time": "2022-09-22T07:22:47.628684",
     "exception": false,
     "start_time": "2022-09-22T07:22:46.502805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import librosa as lr\n",
    "import librosa.feature as lrf\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import librosa as lr\n",
    "import librosa.feature as lrf\n",
    "from scipy.signal import welch\n",
    "import pywt\n",
    "from pywt import *\n",
    "from scipy.signal import periodogram\n",
    "#from pyemd import emd\n",
    "from scipy.signal import hilbert\n",
    "from scipy.stats import linregress, skew, kurtosis\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import nolds\n",
    "from pyentrp import entropy as ent\n",
    "from scipy.signal import detrend\n",
    "from nolds import dfa\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjorth_parameters(eeg_signal):\n",
    "    # Calculate the first derivative (slope) of the EEG signal\n",
    "    diff_signal = np.diff(eeg_signal)\n",
    "    \n",
    "    # Calculate the variance (activity) of the original signal\n",
    "    activity = np.var(eeg_signal)\n",
    "    \n",
    "    # Calculate the variance (activity) of the first derivative (slope)\n",
    "    mobility = np.var(diff_signal)\n",
    "    \n",
    "    # Calculate the mobility parameter (square root of mobility divided by activity)\n",
    "    mobility /= activity\n",
    "    \n",
    "    # Calculate the second derivative of the EEG signal\n",
    "    diff2_signal = np.diff(diff_signal)\n",
    "    \n",
    "    # Calculate the complexity (square root of the mobility divided by the mobility of the first derivative)\n",
    "    complexity = np.sqrt(mobility / (np.var(diff2_signal) / activity))\n",
    "    \n",
    "    return activity, mobility, complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 128\n",
    "n_mfcc = 10 \n",
    "n_chr = 20\n",
    "n_mel = 15\n",
    "n_tonnetz = 15\n",
    "frequency_bands = {\n",
    "    'gamma': (30,64),\n",
    "    'beta': (13, 30),\n",
    "    'alpha': (8, 13),\n",
    "    'theta': (4, 8),\n",
    "    'delta': (1, 4),\n",
    "    }\n",
    "sampling_frequency = 128\n",
    "# Number of sample points\n",
    "N = sr*3\n",
    "# sample spacing\n",
    "T = 1.0 / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b037c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(signal):\n",
    "\n",
    "    feature_vector = {}\n",
    "    \n",
    "    signal -= np.mean(signal)\n",
    "    \n",
    "    # 0 indices are due to array shape\n",
    "    feature_vector['spc_cnt'] = lrf.spectral_centroid(y=signal, sr=sr)[0][0] # Spectral Centroid\n",
    "    feature_vector['spc_roff'] = lrf.spectral_rolloff(y=signal, sr=sr)[0][0] # Rolloff\n",
    "    feature_vector['zc']  = np.array(np.sum(np.abs(np.diff(np.sign(signal)))) / (2 * len(signal)))\n",
    "        \n",
    "    for idx, mfcc in enumerate(lrf.mfcc(y=signal, n_mfcc=n_mfcc, sr=sr)): # First 5 MFCCs\n",
    "        feature_vector['mfcc_' + str(idx)] = mfcc[0]\n",
    "    \n",
    "    for idx, chroma in enumerate(lrf.chroma_stft(y=signal, n_chroma=n_chr, sr=sr)): #chromagram\n",
    "        feature_vector['chr_' + str(idx)] = chroma[0]\n",
    "\n",
    "    for idx, mel in enumerate(lr.power_to_db(lrf.melspectrogram(y=signal, sr=sr))[:n_mel, :]):\n",
    "        feature_vector['mel_' + str(idx)] = mel[0]\n",
    "    \n",
    "    # Iterate over each frequency band\n",
    "    band_powers = {}\n",
    "\n",
    "    # Calculate the power spectral density (PSD) using Welch's method\n",
    "    frequencies, psd = welch(signal, fs=sampling_frequency, nperseg=1024)\n",
    "\n",
    "    # Iterate over each frequency band\n",
    "    # iterations are reversed due to performance differences in certain models\n",
    "    # TODO: reversing process should be improved, way too clunky rn.\n",
    "    for band, (low_freq, high_freq) in reversed(frequency_bands.items()):\n",
    "        # Find indices corresponding to the specified frequency range\n",
    "        band_indices = np.where((frequencies >= low_freq) & (frequencies < high_freq))\n",
    "        # Integrate PSD within the band's frequency range to compute band power\n",
    "        band_power = np.trapz(psd[band_indices], frequencies[band_indices])\n",
    "        band_powers[band] = band_power\n",
    "        feature_vector[band + '_power'] = band_power\n",
    "    \n",
    "    for band in reversed(list(band_powers)):\n",
    "        for child_band in reversed(list(band_powers)):\n",
    "            if child_band == band:\n",
    "                continue\n",
    "            feature_vector[band + '_' + child_band] = band_powers[band]/ band_powers[child_band]\n",
    "        band_powers.pop(band)\n",
    "    \n",
    "    # Calculate the first differences\n",
    "    first_differences = np.diff(signal, n=1)\n",
    "\n",
    "    # Calculate the mean of the absolute values of the first differences\n",
    "    feature_vector['mean_abs_sec_dif'] = np.mean(np.abs(first_differences))\n",
    "\n",
    "    feature_vector['dfa'] = dfa(signal, overlap=False)\n",
    "    \n",
    "    yf = fft(signal)\n",
    "    yf = 2.0/N * np.abs(yf[0:N//2])\n",
    "    np.clip(yf, 0, 15)\n",
    "    yf = (yf - np.min(yf))/(np.max(yf) - np.min(yf))\n",
    "    peaks, _ = scipy.signal.find_peaks(yf, height=0)\n",
    "    peaks, _ = scipy.signal.find_peaks(yf, height=np.max(yf[peaks])*0.25)\n",
    "\n",
    "    xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "    # frequency of the maximum peak    \n",
    "    #feature_vector['peak_freq'] = xf[yf == np.max(yf[peaks])]\n",
    "\n",
    "    # maximum frequency of peaks\n",
    "    #feature_vector['max_freq'] = xf[peaks][len(xf[peaks]) - 1]\n",
    "    \n",
    "    # peak slope\n",
    "    res = linregress(xf[peaks], yf[peaks])\n",
    "    feature_vector['slope'] = res.slope\n",
    "    \n",
    "    feature_vector['skew'] = [skew(signal)][0] #no\n",
    "    feature_vector['kurtosis'] = [kurtosis(signal)][0] #no\n",
    "\n",
    "    activity, mobility, complexity = hjorth_parameters(signal) #no\n",
    "    feature_vector['activity'] = [activity][0]\n",
    "    feature_vector['mobility'] = [mobility][0]\n",
    "    feature_vector['complexity'] = [complexity][0]\n",
    "    feature_vector['rms'] = np.sqrt(np.mean(signal**2))\n",
    "    feature_vector['tempo'] = lr.beat.tempo(y=signal)[0]\n",
    "\n",
    "    \n",
    "    for idx, tonal in enumerate(lrf.tonnetz(y=signal)[:n_tonnetz, :]):\n",
    "        feature_vector['ton_' + str(idx)] = tonal[0]\n",
    "        \n",
    "    return feature_vector\n",
    "    \n",
    "    # rejected features, discuss whether to include them in the paper or not.\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    [cA5, cD5, cD4, cD3, cD2, cD1] = wavedec(signal, 'db1', level=5)\n",
    "    coeffs = [cA5, cD5, cD4, cD3, cD2, cD1]\n",
    "    db1 = pywt.Wavelet('db1')\n",
    "    \n",
    "    re0 = pywt.waverec([coeffs[0]], db1)\n",
    "    re1 = pywt.waverec([coeffs[1]], db1)\n",
    "    re2 = pywt.waverec([coeffs[2]], db1)\n",
    "    re3 = pywt.waverec([coeffs[3]], db1)\n",
    "    re4 = pywt.waverec([coeffs[4]], db1)\n",
    "    re5 = pywt.waverec([coeffs[5]], db1)\n",
    "    '''\n",
    "\n",
    "\n",
    "    # hata veren featureler\n",
    "\n",
    "    '''    \n",
    "    \n",
    "    # Rhythymic Content Features\n",
    "    peaks = -np.sort(-lr.onset.onset_strength(signal, sr = 128))\n",
    "    A0 = peaks[0]\n",
    "    A1 = peaks[1]\n",
    "    RA = A1/A0    \n",
    "    \n",
    "    extracted_features.append(np.mean(lrf.tempogram(signal)))\n",
    "    extracted_features.append(A0)\n",
    "    extracted_features.append(A1)\n",
    "    extracted_features.append(RA)\n",
    "\n",
    "    '''\n",
    "    #feature_vector.append([lr.onset.onset_strength(signal)]) # Flux\n",
    "    #feature_vector.append(lr.feature.zero_crossing_rate(signal)) # Zero Crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89988d84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T07:22:49.135232Z",
     "iopub.status.busy": "2022-09-22T07:22:49.134478Z",
     "iopub.status.idle": "2022-09-22T07:22:49.137611Z",
     "shell.execute_reply": "2022-09-22T07:22:49.138135Z",
     "shell.execute_reply.started": "2022-09-22T07:12:04.812304Z"
    },
    "papermill": {
     "duration": 0.030216,
     "end_time": "2022-09-22T07:22:49.138339",
     "exception": false,
     "start_time": "2022-09-22T07:22:49.108123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_root = 'EEG Data/'\n",
    "def get_data(filename):\n",
    "    '''\n",
    "    Returns the EEG value of a given .mat file.\n",
    "    The value consists of 25 channels, but only 3-16 are EEG channels.\n",
    "    \n",
    "    returns: state, the eeg values with states seperated.\n",
    "    '''\n",
    "    mat = scipy.io.loadmat(os.path.join(data_root, filename))\n",
    "    data = mat['o']['data'][0, 0]\n",
    "    FS = mat['o']['sampFreq'][0][0][0][0]\n",
    "\n",
    "    states = {\n",
    "     'focused': data[:FS * 10 * 60, :],\n",
    "      'unfocused': data[FS * 10 * 60:FS * 20 * 60, :],\n",
    "      'drowsy': data[FS * 20 * 60:FS * 30 * 60, :]\n",
    "    }\n",
    "    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1ba8b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T07:22:49.371399Z",
     "iopub.status.busy": "2022-09-22T07:22:49.370612Z",
     "iopub.status.idle": "2022-09-22T07:22:52.806083Z",
     "shell.execute_reply": "2022-09-22T07:22:52.806608Z",
     "shell.execute_reply.started": "2022-09-22T07:12:04.847409Z"
    },
    "papermill": {
     "duration": 3.463867,
     "end_time": "2022-09-22T07:22:52.806866",
     "exception": false,
     "start_time": "2022-09-22T07:22:49.342999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [06:17<00:00, 47.19s/it]\n",
      "100%|██████████| 8/8 [05:49<00:00, 43.69s/it]\n",
      "100%|██████████| 8/8 [05:42<00:00, 42.78s/it]\n",
      "100%|██████████| 8/8 [06:26<00:00, 48.28s/it]\n",
      "100%|██████████| 8/8 [05:50<00:00, 43.79s/it]\n",
      "100%|██████████| 8/8 [05:32<00:00, 41.59s/it]\n",
      "100%|██████████| 8/8 [05:30<00:00, 41.35s/it]\n",
      "100%|██████████| 8/8 [05:33<00:00, 41.73s/it]\n",
      "100%|██████████| 8/8 [05:31<00:00, 41.46s/it]\n",
      "100%|██████████| 8/8 [05:32<00:00, 41.56s/it]\n",
      "100%|██████████| 8/8 [05:35<00:00, 42.00s/it]\n",
      "100%|██████████| 8/8 [05:34<00:00, 41.84s/it]\n",
      "100%|██████████| 8/8 [05:35<00:00, 41.91s/it]\n",
      "100%|██████████| 8/8 [05:46<00:00, 43.32s/it]\n",
      "100%|██████████| 8/8 [05:41<00:00, 42.71s/it]\n",
      "100%|██████████| 8/8 [05:39<00:00, 42.50s/it]\n",
      "100%|██████████| 8/8 [05:44<00:00, 43.10s/it]\n",
      "100%|██████████| 8/8 [05:34<00:00, 41.83s/it]\n",
      "100%|██████████| 8/8 [05:36<00:00, 42.05s/it]\n",
      "100%|██████████| 8/8 [05:33<00:00, 41.74s/it]\n",
      "100%|██████████| 8/8 [05:33<00:00, 41.70s/it]\n",
      "100%|██████████| 8/8 [05:35<00:00, 41.89s/it]\n",
      "100%|██████████| 8/8 [05:39<00:00, 42.43s/it]\n",
      "100%|██████████| 8/8 [05:35<00:00, 41.98s/it]\n",
      "100%|██████████| 8/8 [05:55<00:00, 44.46s/it]\n",
      "100%|██████████| 8/8 [05:38<00:00, 42.36s/it]\n",
      "100%|██████████| 8/8 [05:36<00:00, 42.03s/it]\n",
      "100%|██████████| 8/8 [05:36<00:00, 42.05s/it]\n",
      "100%|██████████| 8/8 [05:36<00:00, 42.05s/it]\n",
      "100%|██████████| 8/8 [05:32<00:00, 41.61s/it]\n",
      "100%|██████████| 8/8 [05:41<00:00, 42.65s/it]\n",
      "100%|██████████| 8/8 [05:36<00:00, 42.06s/it]\n",
      "100%|██████████| 8/8 [05:39<00:00, 42.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# brute processings\n",
    "channel_indices = np.array(range(3, 17))\n",
    "channel_names = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']\n",
    "channel_names = ['F3', 'F4', 'O1', 'O2', 'F7', 'F8', 'FC5', 'FC6']\n",
    "channel_map = dict(zip(channel_names, channel_indices))\n",
    "\n",
    "rows_list = []\n",
    "from tqdm import tqdm\n",
    "for exp_idx in range(1,35):\n",
    "    if exp_idx == 28: # corrupt file\n",
    "        continue\n",
    "    states = get_data(f\"eeg_record{exp_idx}.mat\")\n",
    "    for ch_name, ch_idx in tqdm(channel_map.items()):\n",
    "        for state, eeg in states.items():\n",
    "            for i in range((eeg.shape[0]//(128*3))):\n",
    "                try:\n",
    "                    data = eeg[i*(128*3):(i+1)*(128*3), ch_idx]\n",
    "                    powers = feature_extraction(data)\n",
    "                    #powers['channel'] = ch_name\n",
    "                    powers['state'] = state\n",
    "                    rows_list.append(powers)\n",
    "                except ValueError:  #raised if `y` is empty.\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d990b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T07:22:52.852914Z",
     "iopub.status.busy": "2022-09-22T07:22:52.851911Z",
     "iopub.status.idle": "2022-09-22T07:22:52.863724Z",
     "shell.execute_reply": "2022-09-22T07:22:52.864356Z",
     "shell.execute_reply.started": "2022-09-22T07:12:07.517878Z"
    },
    "papermill": {
     "duration": 0.036185,
     "end_time": "2022-09-22T07:22:52.864566",
     "exception": false,
     "start_time": "2022-09-22T07:22:52.828381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df = pd.DataFrame.from_dict(rows_list)\n",
    "df = pd.DataFrame.from_records(rows_list).fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbbc77ed",
   "metadata": {
    "papermill": {
     "duration": 0.021212,
     "end_time": "2022-09-22T07:22:52.909210",
     "exception": false,
     "start_time": "2022-09-22T07:22:52.887998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Explore and export data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71358230",
   "metadata": {
    "papermill": {
     "duration": 0.02415,
     "end_time": "2022-09-22T07:22:53.131867",
     "exception": false,
     "start_time": "2022-09-22T07:22:53.107717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Save the features to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b196b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-22T07:22:53.179385Z",
     "iopub.status.busy": "2022-09-22T07:22:53.178658Z",
     "iopub.status.idle": "2022-09-22T07:22:53.192880Z",
     "shell.execute_reply": "2022-09-22T07:22:53.192107Z",
     "shell.execute_reply.started": "2022-09-22T07:12:07.602344Z"
    },
    "papermill": {
     "duration": 0.038465,
     "end_time": "2022-09-22T07:22:53.193047",
     "exception": false,
     "start_time": "2022-09-22T07:22:53.154582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"archive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d834c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('archive.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_state(df, state):\n",
    "    df = df.drop(df[df['state'] == state].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_state = 'unfocused'\n",
    "df = remove_state(df, removed_state)\n",
    "df.loc[df['state'] == 'focused', 'state'] = '1'\n",
    "df.loc[df['state'] == 'drowsy', 'state'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop('state', axis=1)\n",
    "y = df.iloc[:, -1:]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=447)\n",
    "\n",
    "# apply normalization after splitting to avoid leakage\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn as sk\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score,\n",
    "                             classification_report, confusion_matrix, f1_score,\n",
    "                             log_loss, precision_score, recall_score,\n",
    "                             roc_auc_score, roc_curve)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import itertools\n",
    "knn = KNeighborsClassifier(leaf_size= 10)\n",
    "svm = SVC(C=10.0, kernel='rbf', gamma=0.1, random_state=1)\n",
    "xgb = GradientBoostingClassifier(\n",
    "            loss='log_loss', n_estimators=300, learning_rate=0.1, max_depth=10, random_state=1)\n",
    "models = [knn, svm, xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.array(X_train)\n",
    "#y_train = np.array(y_train)\n",
    "#X_test = np.array(X_test)\n",
    "#y_test = np.array(y_test)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "training_acc = knn.score(X_train, y_train)\n",
    "test_acc = knn.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kemalcankucuk/Documents/PAWS Work/eeg-attention/archive/eeg-extract-features.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     training_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mscore(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     test_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(training_acc)\n",
      "File \u001b[0;32m~/Documents/PAWS Work/paws/lib/python3.9/site-packages/sklearn/base.py:706\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 706\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[0;32m~/Documents/PAWS Work/paws/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/PAWS Work/paws/lib/python3.9/site-packages/sklearn/metrics/_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/PAWS Work/paws/lib/python3.9/site-packages/sklearn/metrics/_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[39mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 85\u001b[0m type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my_true\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     86\u001b[0m type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m y_type \u001b[39m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[0;32m~/Documents/PAWS Work/paws/lib/python3.9/site-packages/sklearn/utils/multiclass.py:388\u001b[0m, in \u001b[0;36mtype_of_target\u001b[0;34m(y, input_name)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39m# Check multiclass\u001b[39;00m\n\u001b[1;32m    387\u001b[0m first_row \u001b[39m=\u001b[39m y[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(y) \u001b[39melse\u001b[39;00m y\u001b[39m.\u001b[39mgetrow(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdata\n\u001b[0;32m--> 388\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39;49munique_values(y)\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m (y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(first_row) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    389\u001b[0m     \u001b[39m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m suffix\n\u001b[1;32m    391\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/PAWS Work/paws/lib/python3.9/site-packages/sklearn/utils/_array_api.py:262\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.unique_values\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munique_values\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mreturn\u001b[39;00m numpy\u001b[39m.\u001b[39;49munique(x)\n",
      "File \u001b[0;32m~/Documents/PAWS Work/paws/lib/python3.9/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/PAWS Work/paws/lib/python3.9/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[39m.\u001b[39;49msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[39m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(aux\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    training_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "\n",
    "    print(training_acc)\n",
    "    print(test_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kemalcankucuk/Documents/PAWS Work/eeg-attention/archive/eeg-extract-features.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m grid \u001b[39m=\u001b[39m GridSearchCV(SVC(), param_grid, refit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, verbose \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, n_jobs\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# fitting the model for grid search \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m grid\u001b[39m.\u001b[39mfit(X_train, y_train) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# print how our model looks after hyper-parameter tuning \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#print(grid.best_estimator_) \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kemalcankucuk/Documents/PAWS%20Work/eeg-attention/archive/eeg-extract-features.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m grid_predictions \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "\n",
    "param_grid = {'C': [10],  \n",
    "              'gamma': [0.0359,0.0361,0.0363,0.0365,0.0367,0.0369,0.0371], \n",
    "              'kernel': ['rbf'],\n",
    "              'degree': [3]}  \n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, n_jobs= -1) \n",
    "  \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_train, y_train) \n",
    "  \n",
    "# print how our model looks after hyper-parameter tuning \n",
    "#print(grid.best_estimator_) \n",
    "grid_predictions = grid.predict(X_test)\n",
    "print('Result of the best model on the test set: ', grid_predictions)\n",
    "print(grid.best_params_) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.882514,
   "end_time": "2022-09-22T07:22:54.596071",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-22T07:22:37.713557",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
